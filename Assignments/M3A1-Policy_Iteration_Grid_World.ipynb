{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the gridworld example (Example 4.1 of [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/RLbook2018.pdf)). \n",
    "\n",
    "- State 0 and 15 are the terminal states.\n",
    "- Agent is allowed to move UP, RIGHT, DOWN and LEFT.\n",
    "- Each action deterministically cause the state transitions, except that actions which would take our agent off the grid, in such case the state remains unchanged.\n",
    "- Reward of -1 on all transitions.\n",
    "- Undiscounted and episodic.\n",
    "\n",
    "We will solve this problem i.e. find the optimal oplicy using policy iterations\n",
    "\n",
    "![title](images/M3A1-P1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid_world_environment(object):\n",
    "    def __init__(self, n_row : int = 4, n_col : int = 4) -> None:\n",
    "        '''\n",
    "        @paramenters\n",
    "        n_row  : Number of rows in the grid\n",
    "        n_col  : Number of columns in the grid\n",
    "        \n",
    "        @doc\n",
    "        Initializes the gridword of size (n_row, n_col)\n",
    "        '''\n",
    "        self.n_row = n_row\n",
    "        self.n_col = n_col\n",
    "        self.n_states = self.n_row * self.n_col\n",
    "        self.n_actions = 4\n",
    "        self.states = np.arange(self.n_states)\n",
    "        self.actions = np.arange(self.n_actions) #0:up , 1:right, 2:down, 3:left\n",
    "        self.terminal_states = [0,self.n_states-1]\n",
    "        self.non_terminal_states = list(set(self.states) - set(self.terminal_states))\n",
    "           \n",
    "    def p(self, s :int, a : int, s_dash : int) -> None:\n",
    "        '''\n",
    "        @parameters\n",
    "        s       : current state\n",
    "        a       : action taken\n",
    "        s_dash  : prospective next state\n",
    "        \n",
    "        @doc\n",
    "        Returns the probbaility of transitioning to state 's_dash', if agent takes action 'a' in state 's' \n",
    "        '''\n",
    "        assert a in self.actions\n",
    "        if(a == 0): #Move up\n",
    "            if(s - self.n_col < 0 and s_dash == s):\n",
    "                return 1 \n",
    "            elif(s_dash == s - self.n_col):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        if(a == 1): #Move right\n",
    "            if((s + 1)%self.n_col == 0 and s_dash == s):\n",
    "                return 1\n",
    "            elif((s + 1)%self.n_col != 0 and s_dash == s + 1):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "        if(a == 2): #Move down\n",
    "            if(s + self.n_col >= self.n_states and s_dash == s):\n",
    "                return 1\n",
    "            elif(s_dash == s + self.n_col):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "        if(a == 3): #Move left\n",
    "            if(s%self.n_col == 0 and s_dash == s):\n",
    "                return 1\n",
    "            elif(s%self.n_col != 0 and s_dash == s - 1):\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    def reward(self, s_dash : int) -> float:\n",
    "        '''\n",
    "        @parameter\n",
    "        s_dash    : The state agent is transitioning into\n",
    "        \n",
    "        @doc\n",
    "        Return the reward if agent transtions to state 's_dash'\n",
    "        '''\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/M3A1-P2.png\" alt=\"M4A1-P3\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/M3A1-P3.png\" alt=\"M4A1-P3\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent(object):\n",
    "    def __init__(self, env : grid_world_environment, gamma : float = 1.0, theta : float = 0.001) -> None:\n",
    "        '''\n",
    "        Initializing the agent properties like value function, gamma, theta and the policy. \n",
    "        Initial policy is an equiprobable policy.\n",
    "        '''\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.num_states = self.env.n_states\n",
    "        self.V = np.zeros(self.num_states)\n",
    "        self.policy = dict()\n",
    "        #In the way policy is defined here, there are non-zero probility assigned to some impossible state action pairs.\n",
    "        #But that doesn't matter because anyway are not possible by the environment dynamics, and hence would never be considered.\n",
    "        for state in self.env.non_terminal_states:\n",
    "            for action in self.env.actions:\n",
    "                self.policy[(state, action)] = 1.0/self.env.n_actions\n",
    "        \n",
    "    \n",
    "    def policy_evaluation(self, n_iter : int = 1000) -> None:\n",
    "        '''\n",
    "        In place policy evaluation. Tries to find the true value function within 'n_iter' number of iterations.\n",
    "        '''\n",
    "        for it in range(n_iter):\n",
    "            delta = 0\n",
    "            for state in self.env.non_terminal_states:\n",
    "                value = 0\n",
    "                for action in self.env.actions:\n",
    "                    for state_dash in self.env.states:\n",
    "                        value += (self.policy[(state,action)] * \\\n",
    "                                  self.env.p(state,action,state_dash) * \\\n",
    "                                  (self.env.reward(state_dash) + self.gamma * self.V[state_dash]))\n",
    "                delta = max(delta, abs(self.V[state] - value))\n",
    "                self.V[state] = value\n",
    "            if(delta <= self.theta):\n",
    "                print(\"Policy converged to true value function after {} iterations\".format(it + 1))\n",
    "                self.plot_value()\n",
    "                return\n",
    "        print(\"Policy did not converge to true state value functions.\")\n",
    "        print(self.V)\n",
    "        \n",
    "    \n",
    "    def policy_improvement(self) -> bool:\n",
    "        '''\n",
    "        Policy improvement algorithm. The policy is imporved by acting greedily with repspect to the current value function.\n",
    "        If two actions gives same value, then equal probability is assigned to both of the actions.\n",
    "        Returns True if current policy is same as previous poliy, that is the current policy is the optimal policy. \n",
    "        Else it return false.\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        Implement this\n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def policy_iteration(self, n_policy_iter :int = 100, n_policy_eval_iter : int = 1000) -> None:\n",
    "        '''\n",
    "        Policy iteration algorithm. It calls the policy evaluation step and policy improvement step alternately for a max\n",
    "        of 'n_policy_iter' iterations. \n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        Implement this\n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def plot_value(self) -> None:\n",
    "        '''\n",
    "        Plots the value functions of the sates in the grid.\n",
    "        '''\n",
    "        fig, ax = plt.subplots()\n",
    "        grid_value = np.reshape(self.V , (self.env.n_row, self.env.n_col))\n",
    "        for i in range(self.env.n_col):\n",
    "            for j in range(self.env.n_row):\n",
    "                c = grid_value[j][i]\n",
    "                ax.text(i+0.5, j+0.5, str(round(c,1)), va='center', ha='center')  \n",
    "        ax.set_xlim(0, self.env.n_col)\n",
    "        ax.set_ylim(self.env.n_row, 0)\n",
    "        ax.set_xticks(np.arange(self.env.n_col))\n",
    "        ax.set_yticks(np.arange(self.env.n_row))\n",
    "        ax.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy converged to true value function after 88 iterations\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = grid_world_environment(4,4)\n",
    "ag = agent(env)\n",
    "ag.policy_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = grid_world_environment(4,4)\n",
    "ag = agent(env)\n",
    "ag.policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
